{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. pytorch入门\n",
    "> PyTorch是什么?\n",
    "> 基于Python的科学计算包，服务于以下两种场景:\n",
    "> * 作为NumPy的替代品，可以使用GPU的强大计算能力\n",
    "> * 提供最大的灵活性和高速的深度学习研究平台\n",
    "## 2.1 张量\n",
    "> Tensors(张量)与Numpy中的 ndarrays类似，但是在PyTorch中 Tensors 可以使用GPU进行计算.\n",
    "### 2.1.1 张量初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2562e+01,  4.5824e-41, -1.2562e+01],\n",
      "        [ 4.5824e-41,  4.4842e-44,  0.0000e+00],\n",
      "        [ 1.5695e-43,  0.0000e+00,  9.8451e-34],\n",
      "        [ 0.0000e+00,  2.0319e-43,  0.0000e+00],\n",
      "        [ 1.0659e-33,  0.0000e+00, -1.2561e+01]])\n",
      "tensor([[0.4927, 0.5926, 0.9271],\n",
      "        [0.8014, 0.5490, 0.0220],\n",
      "        [0.2154, 0.9551, 0.4890],\n",
      "        [0.2600, 0.7935, 0.4057],\n",
      "        [0.2638, 0.2972, 0.5310]])\n",
      "tensor([5.5000, 3.0000])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 0.2722, -0.6591, -0.9280],\n",
      "        [ 0.1143,  0.2465, -1.9480],\n",
      "        [-0.4619, -0.8635, -2.2763],\n",
      "        [-0.7010,  0.6364, -0.2351],\n",
      "        [-1.7279,  0.7113,  0.4529]])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "# 创建一个5x3的矩阵, 但是未初始化： 值不一定是0\n",
    "x = torch.empty(5, 3)\n",
    "print(x)\n",
    "# 创建一个随机初始化的矩阵\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "# 创建一个0填充的矩阵， 数据类型为long\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "# 创建tensor并使用现有数据初始化\n",
    "x = torch.tensor([5.5, 3])\n",
    "print(x)\n",
    "# 根据现有的tensor创建tensor, 这些方法将重用输入tensor的属性， 例如， dtype， 除非设置新的值进行覆盖\n",
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "print(x)\n",
    "x = torch.randn_like(x, dtype=torch.float)    # 覆盖dtype!\n",
    "print(x)                                      # 结果size一致"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 获取size\n",
    "> 注：使用size方法与Numpy的shape属性返回的相同，张量也支持shape属性，后面会详细介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.size())\n",
    "torch.Size([5, 3])\n",
    "# torch.Size 返回值是 tuple类型, 所以它支持tuple类型的所有操作."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 操作\n",
    "操作有多种语法。\n",
    "\n",
    "这里仅以加法运算举例\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5484, -0.6246, -0.8582],\n",
      "        [ 1.0425,  0.5433, -1.3128],\n",
      "        [-0.2727, -0.5495, -1.3440],\n",
      "        [-0.4199,  1.5443,  0.2100],\n",
      "        [-1.1336,  0.8240,  1.3279]])\n",
      "tensor([[ 0.5484, -0.6246, -0.8582],\n",
      "        [ 1.0425,  0.5433, -1.3128],\n",
      "        [-0.2727, -0.5495, -1.3440],\n",
      "        [-0.4199,  1.5443,  0.2100],\n",
      "        [-1.1336,  0.8240,  1.3279]])\n",
      "tensor([[ 0.5484, -0.6246, -0.8582],\n",
      "        [ 1.0425,  0.5433, -1.3128],\n",
      "        [-0.2727, -0.5495, -1.3440],\n",
      "        [-0.4199,  1.5443,  0.2100],\n",
      "        [-1.1336,  0.8240,  1.3279]])\n",
      "tensor([[ 0.5484, -0.6246, -0.8582],\n",
      "        [ 1.0425,  0.5433, -1.3128],\n",
      "        [-0.2727, -0.5495, -1.3440],\n",
      "        [-0.4199,  1.5443,  0.2100],\n",
      "        [-1.1336,  0.8240,  1.3279]])\n",
      "tensor([[ 0.2722, -0.6591, -0.9280],\n",
      "        [ 0.1143,  0.2465, -1.9480],\n",
      "        [-0.4619, -0.8635, -2.2763],\n",
      "        [-0.7010,  0.6364, -0.2351],\n",
      "        [-1.7279,  0.7113,  0.4529]])\n",
      "tensor([-0.9280, -1.9480, -2.2763, -0.2351,  0.4529])\n",
      "tensor([-0.4619, -0.8635, -2.2763])\n"
     ]
    }
   ],
   "source": [
    "# 加法1\n",
    "y = torch.rand(5, 3)\n",
    "print(x + y)\n",
    "# 加法2 \n",
    "print(torch.add(x, y))\n",
    "# 加法3: 提供一个输出tensor作为参数\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)\n",
    "# 加法4: 替换操作（in-place）\n",
    "# 注意：任何一个in-place改变张量的操作后面都固定一个_。例如x.copy_(y), x.t_(), 将改变x\n",
    "y.add_(x)\n",
    "print(y)\n",
    "# 你可以使用与NumPy索引方式相同的操作来进行对张量的操作, 这里x[:, 2]表示选择x的第三列元素（从0开始）\n",
    "print(x)\n",
    "print(x[:, 2])\n",
    "print(x[2, :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 view\n",
    "torch.view可以改变张量的维度和大小\n",
    "\n",
    "注：torch.view 与Numpy的reshape类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  #  size -1 从其他维度推断\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2851])\n",
      "0.28509488701820374\n"
     ]
    }
   ],
   "source": [
    "# 如果你有只有一个元素的张量，使用.item()来得到Python数据类型的数值\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 NumPy 转换\n",
    "将一个Torch Tensor转换为NumPy数组是一件轻松的事，反之亦然。\n",
    "\n",
    "Torch Tensor与NumPy数组共享底层内存地址，修改一个会导致另一个的变化。\n",
    "\n",
    "将一个Torch Tensor转换为NumPy数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n",
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# 使用from_numpy自动转换\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意： 所有的 Tensor 类型默认都是基于CPU， CharTensor 类型不支持到 NumPy 的转换. CUDA 张量\n",
    "使用.to 方法 可以将Tensor移动到任何设备中，比如显卡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2851], device='cuda:0')\n",
      "tensor([1.2851], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# is_available 函数判断是否有cuda可以使用\n",
    "# ``torch.device``将张量移动到指定的设备中\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA 设备对象\n",
    "    y = torch.ones_like(x, device=device)  # 直接从GPU创建张量\n",
    "    x = x.to(device)                       # 或者直接使用``.to(\"cuda\")``将张量移动到cuda中\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` 也会对变量的类型做更改"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 自动求导\n",
    "PyTorch 中所有神经网络的核心是 autograd 包。 我们先简单介绍一下这个包，然后训练第一个简单的神经网络。\n",
    "`autograd`包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。\n",
    "\n",
    "### 2.2.1 torch.Tensor\n",
    "`torch.Tensor`是这个包的核心类。如果设置`.requires_grad`为True，那么将会追踪所有对于该张量的操作。当完成计算后通过调用`.backward()`，自动计算所有的梯度，这个张量的所有梯度将会自动积累到`.grad`属性。\n",
    "\n",
    "要阻止张量跟踪历史记录，可以调用`.detach()`方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。\n",
    "\n",
    "为了防止跟踪历史记录（和使用内存），可以将代码块包装在`with torch.no_grad():`中。 在评估模型时特别有用，因为模型可能具有`requires_grad = True`的可训练参数，但是我们不需要梯度计算。\n",
    "\n",
    "在自动梯度计算中还有另外一个重要的类`Function`.\n",
    "\n",
    "`Tensor`和`Function`互相连接并生成一个非循环图，它表示和存储了完整的计算历史。每个张量都有一个`.grad_fn`属性，这个属性引用了一个创建了`Tensor`的`Function`（除非这个张量是用户手动创建的，即，这个张量的`grad_fn是None`）。\n",
    "\n",
    "如果需要计算导数，你可以在`Tensor`上调用`.backward()`。如果`Tensor`是一个标量（即它包含一个元素数据）则不需要为`backward()`指定任何参数， 但是如果它有更多的元素，你需要指定一个`gradient`参数来匹配张量的形状。\n",
    "\n",
    "注：在其他的文章中你可能会看到说将Tensor包裹到Variable中提供自动梯度计算，Variable 这个在0.41版中已经被标注为过期了，现在可以直接使用Tensor，[官方文档](https://pytorch.org/docs/stable/autograd.html#variable-deprecated)\n",
    "\n",
    "具体的后面会有详细说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7fbab8758a90>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7fbab8758a90>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个张量并设置requires_grad=True用来追踪其计算历史\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "# 对张量做一次运算\n",
    "y = x + 2\n",
    "print(y)\n",
    "# y是计算的结果，所以它有grad_fn属性\n",
    "print(y.grad_fn)\n",
    "# 对y做更多操作\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)\n",
    "# .requires_grad_(...) 原地改变了现有张量的 requires_grad 标志。如果没有指定的话，默认输入的这个标志是 False。\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.2 梯度\n",
    "反向传播因为out是一个纯量（scalar），即一个只有一个值的张量。当我们调用out.backward()方法时，PyTorch会自动计算out对所有需要求导的参数的梯度。由于out是一个标量，因此可以直接将 1 作为参数传递给 backward() 方法，也可以不传参数，默认参数为 1，即 out.backward(torch.tensor(1)) 等价于 out.backward()。这样做可以简化代码，特别是在某些情况下，比如计算损失函数时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward0>)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(out)\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到矩阵 4.5.将out叫做Tensor“o”.\n",
    "得到 \n",
    "$o = \\frac{1}{4}\\sum{_iz_i}$, $z_i=3(x_i+2)^2$和$z_i|_{x_i=1}=27$.\n",
    " 因此，$\\frac{\\partial{o}}{\\partial{x_i}}=\\frac{3}{2}(x_i+2)$,则$\\frac{\\partial{0}}{\\partial{x_i}}|_{x_i=1}=\\frac{9}{2}=4.5$.\n",
    "在数学上，如果我们有向量值函数 $\\vec{y}=f(\\vec{x})$ ，且y关于x的梯度是一个雅可比矩阵(Jacobian matrix)：\n",
    "$$\n",
    "J=\\begin{pmatrix}\n",
    "        \\frac{\\partial{y_1}}{\\partial{x_n}} & \\cdots & \\frac{\\partial{y_1}}{\\partial{x_n}} \\\\\n",
    "        \\vdots                              & \\ddots & \\vdots \\\\\n",
    "        \\frac{\\partial{y_m}}{\\partial{x_1}} & \\cdots & \\frac{\\partial{y_m}}{\\partial{x_n}} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "一般来说，torch.autograd就是用来计算vector-Jacobian product的工具。也就是说，给定任一向量$v=(v_1 v_2 \\cdots v_m)^T$，计算 $v^T\\cdot J$，如果v恰好是标量函数$l=g(\\vec{y})$的梯度，也就是说$v=(\\frac{\\partial{l}}{\\partial{y_1}} \\cdots \\frac{\\partial{l}}{\\partial{y_m}})^T$\n",
    "\n",
    "那么根据链式法则，vector-Jacobian product是l关于$\\vec{x}$的梯度：\n",
    "$$\n",
    "J=\\begin{pmatrix}\n",
    "        \\frac{\\partial{y_1}}{\\partial{x_n}} & \\cdots & \\frac{\\partial{y_1}}{\\partial{x_n}} \\\\\n",
    "        \\vdots                              & \\ddots & \\vdots \\\\\n",
    "        \\frac{\\partial{y_m}}{\\partial{x_1}} & \\cdots & \\frac{\\partial{y_m}}{\\partial{x_n}} \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "        \\frac{\\partial{l}}{\\partial{y_1}} \\\\\n",
    "        \\vdots                             \\\\\n",
    "        \\frac{\\partial{l}}{\\partial{y_m}} \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "        \\frac{\\partial{l}}{\\partial{x_1}} \\\\\n",
    "        \\vdots                             \\\\\n",
    "        \\frac{\\partial{l}}{\\partial{x_n}} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "（注意，$v^T\\cdot J$给出了一个行向量，可以通过 $J^T \\cdot v$将其视为列向量）\n",
    "\n",
    "vector-Jacobian product 这种特性使得将外部梯度返回到具有非标量输出的模型变得非常方便。\n",
    "\n",
    "现在让我们来看一个vector-Jacobian product的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 603.5743, -781.9956, -876.0912], grad_fn=<MulBackward0>)\n",
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)\n",
    "# 在这个情形中，y不再是个标量。torch.autograd无法直接计算出完整的雅可比行列，但是如果我们只想要vector-Jacobian product，只需将向量作为参数传入backward：\n",
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "print(x.grad)\n",
    "# 如果.requires_grad=True但是你又不希望进行autograd的计算， 那么可以将变量包裹在 with torch.no_grad()中:\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 神经网络\n",
    "使用torch.nn包来构建神经网络。\n",
    "\n",
    "已经讲过了autograd，nn包依赖autograd包来定义模型并求导。 一个nn.Module包含各个层和一个forward(input)方法，该方法返回output。\n",
    "它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。\n",
    "\n",
    "神经网络的典型训练过程如下：\n",
    "\n",
    "> 1. 定义包含一些可学习的参数(或者叫权重)神经网络模型；\n",
    "> 2. 在数据集上迭代；\n",
    "> 3. 通过神经网络处理输入；\n",
    "> 4. 计算损失(输出结果和正确值的差值大小)；\n",
    "> 5. 将梯度反向传播回网络的参数；\n",
    "> 6. 更新网络的参数，主要使用如下简单的更新原则： `weight = weight - learning_rate * gradient`\n",
    "\n",
    "### 2.3.1 定义网络\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # 卷积层 '1'表示输入图片为单通道, '6'表示输出通道数, '5'表示卷积核为5*5\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # 5*5是通过图片大小和卷积核大小计算得到的\n",
    "        # 全连接层 y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, *args, **kwargs) -> None:\n",
    "        # 卷积 -> 激活 -> 池化\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 如果是方形的话，也可以只写一个数\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # reshape, '-1'表示自适应\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        # 除去批处理维度的其他所有维度\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
