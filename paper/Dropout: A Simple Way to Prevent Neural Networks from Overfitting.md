## 标题 Dropout：一种防止神经网络过拟合的简单方法

## 摘要

过拟合是神经网络训练中常见的问题，它可以导致模型在训练集上表现良好，但在测试集上表现较差。在本文中，我们提出了一种简单的正则化方法，称为 Dropout。Dropout 可以看作是从神经网络中采样子集的过程，随后将其运用于下一层。在训练期间，我们对每个神经元的输出使用 Dropout，以一定的概率将其置零。这样可以使得神经元不能过分依赖其他神经元，从而减少过拟合的风险。我们通过实验验证了 Dropout 的有效性，发现 Dropout 能够显著提高深度神经网络的泛化能力，使其在测试集上的表现更好。

## 1 介绍

深度神经网络包含多个非线性隐藏层，这使得他们非常有表现力的模型，可以学习其输入和输出之间非常复杂的关系。 然而，在有限的训练数据的情况下，这些复杂的关系中的许多将是噪声采样的结果，因此即使从相同的分布中抽取，它们也将存在于训练集中而不是真实的测试数据中。 这导致了过度配合，并已经开发了许多方法来减少它。 这些措施包括：一旦验证组的表现开始变差，就会停止训练，对L1和L2正规化以及轻量级的分配等各种类型的权重惩罚加以惩罚。

在无限的计算中，“正规化”固定大小模型的最佳方法是平均预测所有可能的参数设置，并根据给定训练数据的后验概率对每个设置进行加权。 对于简单的或者小的模型，这有时可以很好地近似，但是我们希望用更少的计算来处理贝叶斯黄金标准的性能。 我们建议通过对参数共享的指数数量的学习模型的预测进行近似的加权几何均值来做到这一点。

模型组合几乎总是提高机器学习方法的性能。 然而，对于大型的神经网络来说，平均许多单独训练的网络输出的显而易见的想法是非常昂贵的。 当各个模型彼此不同时，结合几个模型是最有用的，为了使神经网络模型不同，他们应该有不同的体系结构或者受不同的数据训练。 训练许多不同的体系结构是很困难的，因为为每个体系结构寻找最优的超参数是一项艰巨的任务，训练每个大型网络需要大量的计算。 此外，大型网络通常需要大量的训练数据，可能没有足够的数据在不同的数据子集上训练不同的网络。 即使一个人能够训练许多不同的大型网络，在测试时使用它们也是不可行的。

Dropout是解决这两个问题的技术。 它可以防止过度配置，并提供了一种有效结合指数级多种不同神经网络结构的方法。 术语“Dropout”是指在神经网络中放弃单元（隐藏和可见）。 通过丢弃一个单元，我们的意思是暂时将其从网络中删除，以及所有的输入和输出连接，如图1所示。选择哪个单元是随机的。 在最简单的情况下，每个单位都保持固定的概率p独立于其他单位，其中p可以使用验证集选择或可以简单地设置为0.5，这似乎是接近最佳的广泛的网络和 任务。 然而，对于输入单位来说，保留的最佳概率通常接近1而不是0.5。

![image](https://gitee.com/shawn2gao/myimg/raw/master/2023-5-1015:22:37.png1683703356156.png)

对神经网络应用dropout相当于从中抽取一个“细化”的网络。 细化的网络由所有脱落幸存的单位组成（图1b）。 具有n个单位的神经网络可以被看作是2的n次方个可能的细化神经网络的集合。 这些网络共享权重，因此参数总数仍然是O（n的2次方）或更少。 对于每个训练案例的每一个演示，一个新的细化网络被抽样和训练。 因此，训练一个具有退出的神经网络可以被看作是训练一个具有大量权重共享的2的n次方个细化网络的集合，如果有的话，每个细化的网络得到很少的训练。

在测试时间，对指数级的许多细化模型的预测进行明确的平均是不可行的。然而，一个非常简单的近似平均方法在实践中效果很好。这个想法是在测试时使用单个神经网络，而不会丢失。这个网络的权重是训练权重的缩小版本。如果一个单位在训练期间以概率p被保留，则在测试时间该单位的输出权重乘以p，如图2所示。这确保了对于任何隐藏单位，预期输出（在用于放弃训练单位的分配下时间）与测试时的实际输出相同。通过这样的缩放，具有共享权重的2n个网络可以被组合成单个神经网络以在测试时间被使用。我们发现，训练一个丢失的网络，并在测试时间使用这个近似的平均方法，与其他正则化方法相比，在广泛的分类问题上导致显着较低的泛化误差。

![图1](https://gitee.com/shawn2gao/myimg/raw/master/2023-5-1015:25:22.png1683703522141.png)
dropout的想法不限于前馈神经网络。 它可以更普遍地应用于玻尔兹曼机器等图形模型。 在本文中，我们引入了dropout的限制玻尔兹曼机模型，并将其与标准的限制玻尔兹曼机（RBM）进行比较。 我们的实验表明，在某些方面，辍学RBMs比标准RBMs更好。

此篇文章的结构如下。

第2节，描述了这个想法的动机。

第3节，介绍了以前的相关工作。

第4节，正式描述了dropout模式。

第5节，给出了一个训练dropout网络的算法。

第6节，我们提出我们的实验结果，我们将丢失应用于不同领域的问题，并将其与其他形式的正规化和模型组合进行比较。

第7节，分析了一个神经网络的不同属性dropout的影响，并描述了dropout如何与网络的超参数相互作用。

第8节，介绍了“dropout RBM”模型。

第9节，我们探讨了边缘化dropout的想法。

附录A，我们提供了一个训练dropout网络的实用指南。 这包括对训练dropout网络时选择超参数所涉及的实际考虑的详细分析。

## 2. 动机

dropout的动机来自于性别在进化中作用的理论（Livnat et al。，2010）。 有性繁殖包括取夫妻各自一半的基因，加入非常少量的随机突变，并将它们组合起来产生一个后代。 无性繁殖的选择是创建一个父母的基因略有变异的后代。 似乎合理的是，无性繁殖应该是一个更好的方法来优化个体适应性，因为一组好的基因已经合作到一起可以直接传递到后代。 另一方面，有性生殖可能会破坏这些相互适应的基因组，特别是如果这些集合很大，并且直观地，这会降低已经演变成复杂的复合体的生物体的适宜性。 然而，有性繁殖是最先进的生物进化的方式。

对有性繁殖优越性的一个可能的解释是，从长远来看，自然选择的标准可能不是个体的适应性，而是基因的混合能力。一组基因能够与另一组随机基因一起工作的能力使得它们更加健壮。由于一个基因不能依赖大量的合作伙伴来存在，所以它必须学会独自做一些有用的事情，或者与少数其他基因合作。根据这一理论，有性生殖的作用不仅仅是让有用的新基因在整个人群中传播，还可以通过减少复杂的共同适应来促进这一过程，通过减少一个新基因来提高个体的适应性。类似地，神经网络中的每个隐含单元都要学习与其他单元随机选择的样本一起工作。这应该使每个隐藏单位更加强大，并推动它自己创造有用的功能，而不依靠其他隐藏单位来纠正其错误。然而，一个图层中的隐藏单元仍然会学习彼此做不同的事情。有人可能会设想，“通过制作每个隐藏单元的许多副本，网络将会变得强大，从而避免dropout”，但这是一个糟糕的解决方案，与通过复制代码去处理噪声信道这一糟糕方法的原因完全相同。

进一步讲，dropout这一想法的不同动机来源于思考的谋略。相比于50个人思考一个大的计谋，5个人思考10个计谋显然是一个更好的选择，前提这些人的智商都是一样的。如果条件没有改变，时间固定，一个大阴谋可以很好地工作；但是在非固定的条件下，阴谋越小，工作的机会就越大。一个复杂体的适应性在训练集可能会表现得很好，但是在测试集上，就远不如多个简单体的适应性。

## 3. 相关工作

dropout可以被解释为通过向其隐藏的单位增加噪音来调整神经网络的一种方式。在Vincent等人的去噪自动编码器（DAE）的背景下以及先前已经使用将噪声添加到单元状态的想法。 （2008,2010），噪声被添加到自动编码器的输入单元，通过训练来重建无噪声输入。我们的工作扩展了这个想法，通过显示退出可以有效地应用在隐藏层，也可以被解释为一种模型平均的形式。我们还表明，添加噪声不仅有用于无监督的特征学习，而且还可以扩展到监督学习问题。事实上，我们的方法可以应用于其他基于神经元的架构，例如玻尔兹曼机器。尽管5％的噪声通常对DAE的效果最好，但是我们发现在测试时应用的权重缩放程序使我们能够使用更高的噪声水平。剔除20％的输入单位和50％的隐藏单位往往被认为是最佳的。

## 4. 模型描述

本节介绍了dropout神经网络模型。 考虑具有L个隐藏层的神经网络。设l∈{1，…，L}为网络的隐层提供索引。设z（l）表示输入到第l层的向量，y（l）表示第1层（y（0）= x是输入）的输出向量。 W（1）和b（1）是第1层的权重和偏差。 标准神经网络（图3a）的前馈操作可以描述为（对于l∈{0，…，L-1}和任何隐含的单元i）
没有进行dropout的计算公式：

$$
\begin{aligned}
& \vec{z}_i^{(l+1)} = W_i^{(l+1)}\vec{y}^{(l)} + \vec{b}_i^{(l+1)} \\
& \vec{y}_i^{(l+1)} = f(\vec{z}_i^{(l+1)})
\end{aligned}
$$
公式中的f是任意激活函数，例如:$f(x) = \frac{1}{(1 + exp(−x))}$.
加上dropout之后（图3b）：

$$
\begin{aligned}
& \vec{r}_{j} \sim \operatorname{Bernoulli}(p) \\
& \vec{\~y}^{(l)} = \vec{r}^{(l)} * \vec{y}^{(l)} \\
& \vec{z}_i^{(l+1)} = W_i^{(l+1)} \vec{\~y}^{l}+ \vec{b}_i^{(l+1)} \\
& \vec{y}_i^{(l+1)} = f(\vec{z}_i^{(l+1)})\\
\end{aligned}
$$

我们来看对公式的一种更为直观的描述：
![image](https://gitee.com/shawn2gao/myimg/raw/master/2023-5-1015:55:03.png1683705302382.png)

在dropout公式中， 对于任何层l，r（l）是独立的伯努利随机变量的向量，其中每个随机变量具有概率p为1.该向量被采样并且与该层的输出y（l）元素级地相乘以创建 细化输出y（l）。 然后，已经细化的输出被用作下一层的输入。 这个过程适用于每一层。 这相当于从一个更大的网络中抽取一个子网络。 为了学习，损失函数的导数通过子网络反向传播。 在应用测试集运行中，权重按照 测试时的W（l） = pW（l）来缩放，如图2所示。得到的神经网络没有丢失。

## 5. 学习dropout网络

5.1 反向传播
可以使用随机梯度下降以类似于标准神经网络的方式来训练dropout神经网络。 唯一不同的是，对于小批量(mini-batch)的每个训练案例，我们通过剔除隐藏单元来抽样一个细化的网络。 这个训练案例的前向和后向传播只在这个细化的网络上完成。 在每个小批量的训练案例中，对每个参数的梯度进行平均。 任何不使用参数的训练案例中相应的参数的梯度是0。 许多方法已被用于改善随机梯度下降，如动量，退火学习率（annealed learning rates ）和L2权重衰减。 这些被发现对于dropout神经网络也是有用的。

正则化的一种特殊形式特别适用于dropout ，即限制每个隐藏单元的输入加权向量的范数被固定的常数c所限制。 换句话说，如果w表示映射到任何隐藏单元上的权重矢量，则在约束|| w || 2≤c的情况下对神经网络进行优化。 这个约束是在优化过程中通过将w投射到半径为c的球的表面上而实现的。 这也被称为最大范数正则化，因为它意味着任何权重的规范可以采取的最大值是c。 常数c是可调超参数，它是使用验证集合确定的。 最大规范正则化过去曾用于协作过滤（Srebro和Shraibman，2005）。 它通常会提高深度神经网络的随机梯度下降训练的性能，即使在没有使用dropout的情况下也是如此。

虽然只有dropout本身会有显著性的改善，但是将dropout和最大规范正规化，大衰退的学习率和高动量结合起来比仅仅使用dropout的效果会显著提升。 一个可能的理由是，将权重向量限制在一个固定的半径球内部，使用一个大的学习速率，不会有权重爆炸的可能性。 dropout提供的噪音使得优化过程可以探索权重空间中难以达到的不同区域。 随着学习速度的降低，优化步骤会缩短，从而减少了探索，最终达到最小化。

5.2 无监督的预训练
自动编码器（Vincent等人，2010）或深玻耳兹曼机器（Salakhutdinov和Hinton，2009）可以对神经网络进行预训练。 预训练是利用未标记数据的一种有效方法。 在反向传播的情况下进行预训练已经显示，在某些情况下，随机初始化可以显着提高性能。

dropout可以应用于已经使用这些技术预训练的网络。 预训练过程保持不变。 预训练所得的重量应按1 / p的比例放大。 这确保了对于每个单元，随机dropout期间的预期输出将与预训练期间的输出相同。 我们最初担心的是，dropout的随机性可能会抹去预训练权重中的信息。 这种情况发生时，精细调整期间使用的学习率与随机初始化网络的最佳学习率相当。 然而，当学习率被选择为较小时，预训练权重的信息似乎被保留了下来，并且在最终的泛化错误方面我们能够得到改善，而不是在微调网络时使用dropout。

6. 实验结果
我们在不同领域的数据集上训练了dropout神经网络来分类问题。 我们发现，与没有使用dropout的神经网络相比，dropout改善了所有数据集的泛化性能。 表1给出了数据集的简要说明。 数据集是:
• MNIST : 手写数字的标准玩具数据集。
• TIMIT : 清晰的用于语音识别的标准语音基准集。
• CIFAR-10 and CIFAR-100 : 微小的自然图像 (Krizhevsky, 2009).
• Street View House Numbers data set (SVHN) : Google Street View收集的房屋号码的图像 (Netzer et al., 2011).
• ImageNet : 大量的自然图像。
• Reuters-RCV1 : 路透社的新闻文章的数据集。
• Alternative Splicing data set: 用于预测替代基因剪接的RNA特征(Xiong et al., 2011).
我们选择了一组不同的数据集来证明dropout是一种改进神经网络的通用技术，并不是特定于任何特定的应用领域。 在本节中，我们提出了一些显示退出的有效性的关键结果。 附录B提供了所有实验和数据集的更详细的描述。
表1

6.1 图像数据集的结果
我们使用五个图像数据集来评估丢失MNIST，SVHN，CIFAR-10，CIFAR-100和ImageNet。 这些数据集包括不同的图像类型和训练集大小。 在所有这些数据集上获得最新结果的模型都使用了dropout。

6.1.1 MNIST
表2
MNIST数据集由28×28像素的手写数字图像组成。任务是将图像分类成10位数的类别。表2比较了dropout与其他技术的表现。对于不使用dropout或无监督预训练的设置，表现最好的神经网络达到约1.60％的误差（Simard等人，2003）。使用dropout，错误降低到1.35％。用ReLUs代替线性回归（Jarrett et al。，2009）进一步将误差降低到1.25％。最大范数正则化再次降低到1.06％。增加网络的大小可以带来更好的结果。每层2层8192个单位的神经网络误差为0.95％。请注意，这个网络有超过6500万个参数，正在接受一个大小为60,000的数据集的培训。使用标准的正则化方法和早期停止训练一个这样大小的网络来给出良好的泛化误差是非常困难的。另一方面，dropout，即使在这种情况下，也可以防止过拟合。它甚至不需要提前停止。 Goodfellow等人（2013年）显示，通过用Maxout单位取代ReLU单位，结果可以进一步提高到0.94％。所有的丢失网络对于隐藏单位使用p = 0.5，对于输入单位使用p = 0.8。附录B.1中提供了更多的实验细节。

用RBM和深玻尔兹曼机器叠加的dropout网络也给出了改进，如表2所示。DBM预先训练的dropout网络实现了0.79％的测试误差，这是有史以来就置换不变设置所报告的最佳性能。 我们注意到，通过使用二维空间信息和从标准训练集中增加具有畸变版本图像的训练集可以获得更好的结果。 我们在更有趣的数据集上展示了这个设置中的dropout的有效性。

为了测试dropout的鲁棒性，分类实验是用许多不同架构的网络来完成的，保持所有的超参数（包括p）固定。 图4显示了随着培训的进展，这些不同架构的测试错误率。 训练有dropout和无dropout的相同体系结构具有显着不同的测试误差，如由两个单独的轨迹群所看到的那样。 在所有体系结构中，Dropout提供了巨大的改进，而不使用针对每个体系结构进行特定调整的超参数。
图4

6.1.2 Street View House Numbers（SVHN）
街景房屋号码（SVHN）数据集（Netzer et al。，2011）由谷歌街景收集的房屋号码的彩色图像组成。 图5a显示了这个数据集的图像的一些例子。 我们在实验中使用的数据集的一部分包括32×32彩色图像，大致集中在一个门牌号的数字上。 任务是确定这个数字。

表3

对于这个数据集，我们将Dropout应用于卷积神经网络（LeCun et al。，1989）。我们发现的最好的架构有三个卷积层，其次是两个完全连接的隐藏层。所有隐藏的单位是ReLUs。每个卷积层之后是最大池化。附录B.2更详细地描述了架构。对于网络的不同层，保留隐藏单元的概率为p =（0.9,0.75,0.75,0.5,0.5,0.5）（从输入到卷积层到完全连接层）。最大范数正则化被用于卷积和完全连接层的权重。表3比较了不同方法获得的结果。我们发现卷积网络胜过其他方法。不使用丢失的性能最好的卷积网络实现了3.95％的错误率。仅向完全连接的图层添加压差将误差降低到3.02％。为卷积层添加Dropout，进一步将错误降低到2.55％。通过使用maxout单元可以获得更多的收益。

通过在卷积层中添加Dropout（3.02％至2.55％）获得的性能的额外增益值得注意。有人可能认为，由于卷积层没有很多参数，因此过度拟合不是问题，因此Dropout不会有太多的影响。然而，在较低层中的Dropout仍然有帮助，因为它为较高的全连接层提供了噪声输入，从而防止它们过拟合。

6.1.3 CIFAR-10 and CIFAR-100
CIFAR-10和CIFAR-100数据集由分别来自10个和100个类别的32×32个彩色图像组成。 图5b显示了这个数据集的图像的一些例子。 附录B.3给出了数据集，输入预处理，网络结构和其他实验细节的详细描述。 表4显示了通过这些数据集上的不同方法获得的错误率。 没有任何数据增加，Snoek et al。 （2012）使用贝叶斯超参数优化在CIFAR-10上获得了14.98％的错误率。 在完全连接的层中使用dropout将其降低到14.32％，并且在每层中增加dropout进一步将错误降低到12.61％。 Goodfellow等人 （2013）表明，通过用Maxout单元替换ReLU单元，误差进一步降低到11.68％。 在CIFAR-100上，退出将误差从43.48％降低到37.20％，这是一个巨大的改进。
图5
表4

6.1.4 ImageNet
ImageNet是一个超过1500万标记的高分辨率图像数据集，属于大约22000个类别。从2010年开始，作为Pascal视觉对象挑战赛的一部分，每年举办一次名为ImageNet大型视觉识别挑战赛（ILSVRC）的比赛。在这个挑战中使用了ImageNet的一个子集，1000个类别中大概有1000个图像。由于类别的数量相当大，因此通常报告两个错误率：top-1和top-5，其中top-5的错误率是测试图像的分数，正确的标签不是在五个标签之间这个模型很可能被认为是可能的。图6显示了我们的模型在一些测试图像上做出的一些预测。
ILSVRC-2010是ILSVRC唯一可用的测试集标签版本，所以我们大部分的实验都是在这个数据集上进行的。表5比较了不同方法的性能。具有dropout的卷积网大大优于其他方法。 Krizhevsky等人详细描述了架构和实现细节。 （2012年）。
图6
表5
表6
我们基于卷积网络和dropout的模型赢得了ILSVRC-2012的竞赛。 由于测试集的标签不可用，因此我们将结果报告在最终提交的测试集上，并包含我们模型的不同变体的验证集结果。 表6显示了比赛的结果。 虽然基于标准视觉特征的最佳方法实现了大约26％的前5个错误率，但是具有dropout的卷积网络实现了大约16％的测试误差，这是惊人的差异。 图6显示了我们的模型预测的一些例子。 我们可以看到，模型做出了非常合理的预测，即使最好的猜测是不正确的。

6.2 在TIMIT上的实验结果
接下来，我们将dropout应用于语音识别任务。 我们使用TIMIT数据集，该数据集由680位发言者的录音组成，涵盖了美国英语的8种主要方言，在受控制的无噪音环境下阅读10个语音丰富的句子。 在21个对数滤波器组的窗口上训练dropout神经网络，以预测中心帧的标签。 附录B.4描述了数据预处理和训练细节。 表7比较了dropout神经网络与其他模型。 6层网络的声音出错率为23.4％。 dropout率进一步提高到21.8％。 我们还训练了从训练后体重开始的dropout网络。 预先加入一叠RBM的4层网络的声音出错率为22.7％。dropout率降低到19.7％。 同样，对于一个8层网络，误差从20.5％降到19.7％。
表7

6.3 在文字数据集上的实验结果
为了测试文本域中dropout的有用性，我们使用了dropout网络来训练文档分类器。 我们使用了Reuters-RCV1数据集的一个子集，收集了来自路透社的超过800,000篇newswire文章。 这些文章涵盖了各种主题。 任务是拿一些文档的文字表示，并把它分成50个不相交的主题。 附录B.5更详细地描述了设置。 我们最好的没有使用dropout的神经网络获得了31.05％的错误率。 添加dropout将错误降低到29.62％。 我们发现，与视觉和语音数据集相比，这一改进要小得多。

6.4 与贝叶斯神经网络的比较
dropout可以被看作是对具有共享权重的指数级许多模型进行等权重平均的一种方式。另一方面，贝叶斯神经网络（Neal，1996）是在神经网络结构和参数空间上进行模型平均的正确方法。在dropout时，每个模型的权重是相等的，而在贝叶斯神经网络中，每个模型都要考虑到以前的模型以及模型如何处理数据，这是更正确的方法。贝叶斯神经网络对于解决数据稀缺的领域（如医学诊断，遗传学，药物发现和其他计算生物学应用）中的问题非常有用。然而，贝叶斯神经网络训练缓慢，难以扩展到非常大的网络规模。此外，在测试时间从许多大网络获得预测是昂贵的。另一方面，dropout神经网络在测试时间训练和使用要快得多。在本节中，我们报告将贝叶斯神经网络与dropout神经网络在贝叶斯神经网络已知性能良好并获得最新结果的小数据集上进行比较的实验。目的是分析贝叶斯神经网络与使用dropout神经网络的对比损失。

我们使用的数据集（Xiong et al。，2011）来自遗传学领域。其任务是根据RNA特征来预测选择性剪接的发生。选择性剪接是哺乳动物组织细胞多样性的重要原因。预测在不同条件下某些组织中交替剪接的发生对于理解许多人类疾病是重要的。鉴于RNA特征，任务是预测生物学家关心的三个剪接相关事件的概率。评估指标是代码质量，它是目标与预测概率分布之间的负KL散度的量度（越高越好）。附录B.6包括数据集和性能指标的详细描述。

表8总结了这个数据集上不同模型的性能。Xiong等人（2011）使用贝叶斯神经网络来完成这项任务。正如预期的那样，我们发现贝叶斯神经网络表现比dropout更好。然而，我们看到dropout显着地改善了标准神经网络的性能，并且胜过了所有其他的方法。这个数据集的挑战是防止过拟合，因为训练集的规模很小。防止过拟合的一种方法是使用PCA降低输入维数。此后，可以使用标准技术如SVM或逻辑回归。但是，在dropout的情况下，我们能够防止过拟合，而不需要降低维度。与贝叶斯网络中的几十个单元相比，dropout网络非常大（隐藏单元为1000个）。这表明dropout有很强的正规化效应。

表8

6.4 与标准正则化的比较
已经提出了几种正则化方法来防止神经网络中的过拟合。 这些包括L2权重衰减（更一般的Tikhonov正则化（Tikhonov，1943））， lasso （Tibshirani，1996），KL-稀疏性和最大范数正则化。 dropout可以被看作是规范神经网络的另一种方式。 在本节中，我们使用MNIST数据集比较了这些正则化方法中的一些丢失。

具有ReLU的相同网络体系结构（784-1024-1024-2048-10）使用具有不同正则化的随机梯度下降进行训练。 表9显示了结果。 使用验证集合获得与每种正则化（衰减常数，目标稀疏性，dropout率，最大范数上限）相关的不同超参数的值。 我们发现，dropout结合最大范数正则化给出了最低的泛化误差。
表9
